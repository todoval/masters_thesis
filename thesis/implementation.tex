\chapter{Implementation} \label{chap:implementation}

We approach the problem of spectral uplifting similarly to~\citet{upsamplingJakobHanika}, where an uplifting model is created prior to rendering. Our implementation therefore consists of two parts --- \emph{model creation} and its subsequent \emph{utilization} in a rendering software. 

For the first part, we extend an already existing uplifting tool, the \emph{Borgtool}, which is currently used for creating sigmoid-based RGB cubes in the same manner as in~\cref{alg:upliftingAlgSigmoid}. We add the possibility for creating trigonometric moment-based cubes (from now on referred to as \emph{trigonometric moment cube}), i.e. for the spectra to be stored with trigonometric moments rather than sigmoid coefficients. We also add an option for constraining such a cube with a user-specified atlas.

We then show the performance of such a model by integrating it in ART, which, up until now, used only one built-in sigmoid-based cube for all its uplifting processes.

\section{Uplifting model}

The core of this section is the already mentioned Borgtool. It is a stand-alone, non-open source thing that was created by Weta and is .

The output of the Borgtool is an RGB cube structure which contains multiple entries in form of lattice points. Following, we name the main parameters of a single cube entry of the already supported sigmoid-based cube:
\begin{itemize}
	\item \emph{target RGB} --- the RGB coordinates that the point has in the cube
	\item \emph{coefficients} --- 3 sigmoid coefficients used to reconstruct a spectrum so it matches the target RGB
	\item \emph{lattice RGB} --- the actual RGB that the reconstructed spectrum evaluates to. Ideally, this should match the target RGB
\end{itemize}
Along with its entries, the resulting cube structure also stores a few other properties, both \emph{static}, such as the illuminant according to which the RGB cube is uplifted, and \emph{user-adjustable}, such as the cube dimension or the fitting threshold (i.e. the maximum allowed difference between the target and the lattice RGB).

Our trigonometric moment cube can be viewed as an extension of the sigmoid cube, as it keeps most of its parameters and mainly extends the already existing ones. The main difference between them lies in the distinction of the lattice points --- while the sigmoid cube regards all of its points as equal, the trigonometric moment cube distinguishes (by means of a \texttt{seeded} boolean parameter) between \emph{atlas lattice points}, i.e. the lattice points that store the user-inputted RGB:spectra mappings; and \emph{regular points}, which do not.

Our requirements for the shape of the spectra at atlas lattice points differ from the ones at regular lattice points. While we prefer the regular lattice points to have their spectra as smooth as possible in order to avoid unexpected artifacts under other illuminants, the coefficients of the atlas lattice points must reconstruct spectra almost identical to the input spectra, which might include sharp edges and spikes.

Therefore, it is sufficient for the regular lattice points to be represented with a smaller number of coefficients, while atlas lattice points might require a lot more. Although a smooth spectrum can be represented with a high number of coefficients, such a representation is memory inefficient, its reconstruction is more time consuming, and, most importantly, it does not work well with the optimizer. Based on our experiments in~\cref{ssec:noOfMoments}, we decide to store the spectra of regular lattice points with 3 coefficients and adjust the number of coefficients of the atlas lattice points depending on the nature of its desired spectrum.

In addition to supporting variable number of coefficients (ranging from 3 to 24), the trigonometric moment cube also supports the possibility of a multiple coefficient representations per lattice point. The sole purpose of such extension is to lower the cube size requirements upon constraining, which we address more thoroughly in ref.

In addition to the cube structure, the uplifting process of the trigonometric moment cube is also similar to the one of the sigmoid cube, which closely follows~\cref{alg:upliftingAlgSigmoid}. Its main distinctions are in the constraining process (which the sigmoid cube lacks) and in the first round of optimizing, or, as we refer to from now on, \emph{fitting}.

Following, we name the individual steps of the process, which we then analyze in greater detail.
\begin{enumerate}
	\item \emph{Initialization}
	\item \emph{Cube seeding (optional)}
	\item \emph{Fitting of starting points}
	\item \emph{Cube fitting}
	\item \emph{Cube improvement}
	\item \emph{Cube storage}
\end{enumerate}

\subsection{Initialization} \label{ssec:initialization}

This part of the run is responsible for three things:
\begin{itemize}
	\item parsing of the parameters
	\item initialization of the cube and its entries with default values
	\item loading of the required color atlases
\end{itemize}

The initialization of the cube is pretty straightforward, as all of its properties are either user-defined or set to default (note: the default illuminant is always D65). The number of cube entries is directly proportional to the cube's \texttt{dimension} parameter, which specifies the number of entries per one axis. This renders the total number of entries to $dimension^3$. As the lattice points are positioned evenly, their target RGB values are equivalent to their coordinates in the RGB cube.

The loading of the atlases is a bit more complicated. Firstly, a single color atlas is inputted in a form of a simple .txt file, which contains merely a list of entries in a textual form as shown in~\cref{fig:macbethSampleText}. Therefore, it requires parsing.

\begin{figure}
	\lstset{
		string=[s]{"}{"},
		comment=[l]{:},
		commentstyle=\color{black},
		basicstyle=\scriptsize
	}
	\begin{lstlisting}[label=lst:atlasEntry]
	Entry ID:   orange
	------------------------------------------------------------------------
	Description           :  "orange" patch of the Macbeth colour checker
	Type                  :  reflectance spectrum
	Fluorescence data     :  no
	Measurement device    :  
	Measured by           :  
	Measurement date      :  
	
	Sampling information
	--------------------
	Type	    	      :  regular
	Start                 :  380.0 nm
	Increment             :  5.0 nm
	Maximum sample value  :  100.0
	
	ASCII sample data
	-----------------
	{6.143748,  5.192119,  4.867970,  5.092529,  4.717562,  4.663087, 
	4.455331,  4.562958,  4.517197,  4.536289,
	4.454180,  4.543101,  4.491708, ... }
	\end{lstlisting}
	\caption{A sample entry from the Macbeth Color Checker atlas}
	\label{fig:macbethSampleText}
\end{figure}

Moreover, the spectral data obtained from the atlases cannot be stored in the Borgtool directly, so as to avoid extreme memory requirements arising with large atlases. To solve this problem, we take advantage of the trigonometric moments.

We store the spectral curves of the individual atlas entries with Fourier coefficients as described in~\cref{par:spectrumToCoefficientConversion}. We mirror but do not warp the signal prior to coefficient computation (see~\cref{sec:storingMoments}).

The number of coefficients per atlas entry is variable, ranging from 4 to 24. We explain our method for determining the sufficiency of coefficient representation, and therefore the coefficient count for each atlas entry, in~\cref{ssec:noOfMoments}.

\subsection{Cube seeding} 

In order to uplift the whole cube as described in~\cref{alg:upliftingAlgSigmoid}, we must first fit one or more \emph{starting points}, whose coefficients can then be used as prior for the fitting of other lattice points. For these purposes, we utilize the user-specified color atlas. The general idea behind this process is to copy the coefficients of atlas entries to specific lattice points, and then use these coefficients as prior for fitting said lattice points. We refer to this process as \emph{seeding}, and term the seeded lattice points \emph{atlas lattice points}.

The ideal scenario would be if the RGB of the atlas entries were to perfectly match the coordinates of lattice points. However, as the RGB value of an atlas entry can be virtually any triplet within the (0,1) range, it is most likely that the atlas entries evaluate to a point inside the cube voxel.

A realistic approach would be to create a complete injective mapping between the atlas entries and their closest lattice points. However, such a mapping would provide satisfactory results only if we were to use the nearest-neighbor method for uplifting non-mapped RGB triplets. If we were to use either the interpolation of coefficients or spectra, which both employ all 8 voxel corners during the uplift, the curves of uplifted RGB values of atlas entries would most likely be considerably distinct from the desired result (i.e. the original atlas entry spectrum), subsequently causing color artifacts. We can observe this behavior in~\cref{fig:seedingMethod1corner}. However, as seen in~\cref{fig:seedingMethod8corners}, propagating the information about the original reflectance of the entry to all voxel corners improves the result remarkably. We therefore opt for seeding all 8 voxel corners per an atlas entry.

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.54\textwidth}
		\includegraphics[width=\linewidth]{img/seeding_method_legend.png}
	\end{subfigure} \\
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth,height=0.2\textheight]{img/seeding_method_1corner.png}
		\caption{Only 1 voxel corner seeded}
		\label{fig:seedingMethod1corner}
	\end{subfigure} \hspace{0.1em}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{img/seeding_method_8corners.png}
		\caption{All 8 voxel corners seeded}
		\label{fig:seedingMethod8corners}
	\end{subfigure}
	\caption{Comparison of the performance of two seeding methods}
	\label{fig:seedingMethodInterpolation}
\end{figure}

During the seeding process, it may occur that two atlas entries would fall into neighboring voxels, i.e. that they would share some of the voxel corners. We solve such collisions by utilizing the possibility of one lattice point having multiple coefficient representations. In addition to coefficients and their count, we also store an entry ID per each representation, so as to later distinguish the reconstructed curves during rendering and decide which to employ, which we explain in more detail in ref.

If two atlas entries fall into the same voxel, however, there is no way to determine the interpolation of which the user desires upon uplifting the RGB values inside said voxel. We therefore discard one of the entries and throw an error informing the user of the collision and suggesting to increase of the cube dimension.

We show an example of a cube seeded and subsequently fitted with the Munsell Book of Colour in~\cref{fig:seededCubeMCB}. Lattice points marked as black represent the seeded points. Note that a lot of these points store multiple coefficient representations.

\begin{figure}[t!]
	\centering
	\captionsetup[subfigure]{font=footnotesize,labelfont=footnotesize}
	\captionsetup[subfigure]{justification=centering}
	\begin{subfigure}[t]{0.22\textwidth}
		\includegraphics[width=\linewidth]{img/seededCube_mcb1.png}
		\label{fig:seededCube_mcb1}
	\end{subfigure} \hspace{0.05em}
	\begin{subfigure}[t]{0.22\textwidth}
		\includegraphics[width=\linewidth]{img/seededCube_mcb2.png}
		\label{fig:seededCube_mcb2}
	\end{subfigure} \hspace{0.05em}
	\begin{subfigure}[t]{0.22\textwidth}
		\includegraphics[width=\linewidth]{img/seededCube_mcb3.png}
		\label{fig:seededCube_mcb3}
	\end{subfigure} \hspace{0.05em}
	\begin{subfigure}[t]{0.22\textwidth}
		\includegraphics[width=\linewidth]{img/seededCube_mcb4.png}
		\label{fig:seededCube_mcb4}
	\end{subfigure}
	\caption{A 32-dimensional cube fitted with the Munsell Book of Colour}
	\label{fig:seededCubeMCB}
\end{figure}

Constraining the uplifting process with an atlas is optional. If no atlas is inputted, all lattice points are regarded as regular lattice points and the cube is fitted from the middle in the same manner as the sigmoid cube. However, the resulting uplifting structure provides no advantages over the sigmoid cube.

Supporting this option requires us to specify 3 prior coefficients for the center point, i.e. for a lattice points whose spectral curve roughly evaluates to an RGB of $(0.5, 0.5, 0.5)$. By storing spectral curves that roughly evaluate to such RGB with the trigonometric moments, we observe that the coefficients roughly evaluate to $\{0.5, 0, 0\}$. Therefore, we use them as prior.

We provide a comparison between the starting points when seeding from the middle (either with our or the sigmoid method) and when seeding with an atlas in~\cref{fig:seededStartingPoints}, so as to have a rough idea of their placement.

\begin{figure}[t]
	\centering
	\captionsetup[subfigure]{font=footnotesize,labelfont=footnotesize}
	\captionsetup[subfigure]{justification=centering}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{img/seededStarting_sigmoid.png}
		\label{fig:seededStarting_sigmoid}
	\end{subfigure} \hspace{0.2em}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{img/seededStarting_mcb.png}
		\label{fig:seededStarting_mcb}
	\end{subfigure}
	\caption{Comparison of the placement of starting points when seeding with the Munsell Book of Colour (right) and when fitting from the middle with sigmoids (left)}
	\label{fig:seededStartingPoints}
\end{figure}

\subsection{Fitting of starting points} \label{ssec:startingPointsFitting}

By seeding the cube, we have appointed coefficients to some of the lattice points. These coefficients reconstruct a spectrum that evaluates to an RGB value which we denote as the \emph{lattice RGB}. The difference between the lattice and the target RGB is therefore equal to the distance between the lattice point and its assigned atlas entry in the cube. It is apparent that the distance may be higher than the defined fitting threshold. In such cases, we must ``improve'' upon the coefficients so that the resulting color difference is as low as possible.

Our problem of improving the coefficients satisfies the definition of the \emph{Non-linear Least Squares} problem~\cite{nonLinearLeastSquares}. Non-linear Least Squares is an unconstrained minimization problem in the following form:
\begin{equation} \label{eq:nonLinearLeastSquares}
	 \underset{x}{\text{minimize}} \hspace{0.5em} f(x) = \sum_{i} f_{i}(x)^{2},
\end{equation}
where $x= \{x_{0}, x_{1}, x_{2}, ... \}$ is a parameter block that we are trying to improve (i.e. our coefficients) and $f_{i}$ are so-called \emph{cost functions}. The definition of cost functions is dependent solely on the current problem. In our case, we primarily require to minimize the difference between the lattice and the target RGB. Our secondary requirement is for the shapes of the original atlas entry curve and the reflectance curve of the atlas lattice point to be similar. This gives rise to multiple choices for cost functions, such as using the difference between curves along with the Delta E error, using one or multiple cost functions for the RGB error etc\ldots After implementing some of them and testing their performance, the results of which we provide in~\cref{ssec:costFunctions}, we decided on using four cost functions --- three for specifying the absolute difference in one of the three axes of the cube, and one for specifying the average distance between curves per wavelength sample. We also include a heuristic which sets the value of the fourth cost function to 0 if the curve distance falls below a certain threshold, and iteratively increases the threshold if the optimizer fails. The reasoning behind this is also explained in~\cref{ssec:costFunctions}.

To solve an optimization problem defined in the way as described above, we use the CERES solver. We 

\subsubsection{CERES solver} \label{sssec:ceresSolver}

As already mentioned in~\cref{sec:upliftingMethods}, the CERES solver is an open-source library for solving large optimization problems such as our Non-linear Least Squares problem. It consists of two parts --- a \emph{modeling API} which provides tools for the construction of optimization problems, allowing us to set parameters such as maximum number of iterations of the optimizer or maximum number of consecutive nonmononotic steps; and a \emph{solver API} that controls the minimization algorithm.

To solve a Non-linear Least Squares problem, the solver requires us to specify only a so-called \emph{residual block}, which is a structure defined by the prior coefficients and the cost functions. During the execution, the solver attempts to minimize the values of the cost functions (or \emph{residuals}) in the residual block. The execution is aborted and the current best parameter block returned when the solver achieves either the specified number of iterations or nonmonotonic steps. For more information on the specifics of the CERES solver, we refer the interested reader to its documentation by~\citet{ceresNonLinearLeastSquares}.

There is one main downside to using the CERES solver. As it was designed to handle very large, sparse problems where every residual term depends on only a few of the input parameters, it is not ideal for solving problems with only one large parameter block, i.e. it might get stuck in local minima and therefore produce unsatisfactory results. Unfortunately, if an atlas lattice point is represented with a high number of coefficients, our optimization problem falls into this category.

We solve such problematic cases by applying a simple heuristic, which consists of slightly altering the first coefficient (as it has the highest influence on the shape of the curve) and running the optimizer again. However, although such an optimization greatly improves the overall performance of fitting, it remains insufficient for too high a number of coefficients, i.e. the threshold for the fourth residual must be increased to values extremely high and, by then, it loses resemblance to the original shape.

Therefore, we implement another heuristic improvement --- if the coefficient count is higher than 14, we let the optimizer optimize only the first 4 coefficients while leaving the others constant. We use the threshold of $c > 14$ as that is roughly the boundary where the fitted curves begin to show undesired artifacts, and we optimize the first 4 coefficients because their number is both low enough for the optimizer to handle without errors, and high enough so we give the fitting process enough freedom without it having to resort to sinusodial-like shapes.

We summarize the fitting process of the starting points, including the utilization of threshold for our fourth cost function (see~\cref{ssec:costFunctions}), in~\cref{alg:fitting_alp}. If the optimizer is unable to fit an atlas lattice points, we throw a warning and convert it into a regular lattice point. However, as of yet, we have not encountered a failure.

\begin{algorithm}[t!]
	\caption{Fitting of one coefficient representation of a $point$ from atlas lattice points}
	\label{alg:fitting_alp}
	\begin{algorithmic}[1]
		\State $threshold \gets 0.001$
		\While {$threshold < 1$}
		\State $coefsToFit \gets$ either the first 4 coefficients or all of them depending on the coefficient count of $point$
		\State $i \gets 0$
		\While optimizer is unsuccessful \textbf{and} $i < maxIterations$
		\State heuristically change $coefsToFit[0]$
		\State run the optimizer with parameters $coefsToFit$ and threshold set to $threshold$
		\State $i++$
		\EndWhile
		\If{optimizer was successful}
		\State $point.coefs \gets coefsToFit$
		\State break
		\EndIf
		\State increase $threshold$
		\EndWhile
	\end{algorithmic}
\end{algorithm}

\subsection{Cube fitting} \label{ssec:cubeFitting}

As the atlas lattice points are represented with a higher number of coefficients and may even contain multiple coefficient representations, they cannot be directly used as prior for the regular lattice points. First, they must be ``converted'' into a lower-dimensional representation.

We refer to the conversion process as \emph{coefficient recalculation}. It consists of reconstructing the reflectance spectrum of the atlas lattice point and subsequently saving it with 3 coefficients. Although this process causes significant loss of spectral information, it preserves the rough outline of the curve. This works to our benefit --- it reduces the likelihood of significant color artifacts between the atlas lattice points and regular lattice points while keeping the spectra smooth.


\begin{figure}[t!]
	\centering
	\captionsetup[subfigure]{font=footnotesize,labelfont=footnotesize}
	\includegraphics[width=0.8\linewidth]{img/recalculation.png}
	\caption{Comparison of coefficient recalculation techniques. Left: first fitted entry; right: second fitted entry; middle: result of recalculation}
	\label{fig:test}
\end{figure}

\begin{figure}[t!]
	\centering
	\captionsetup[subfigure]{font=footnotesize,labelfont=footnotesize}
	\captionsetup[subfigure]{justification=centering}
	\begin{subfigure}[t]{0.30\textwidth}
		\includegraphics[width=1\linewidth,height=4em]{img/recalculation_color_green.png}
		\includegraphics[width=1\linewidth,height=4em]{img/recalculation_color_fitGreen.png}
		\includegraphics[width=1\linewidth,height=4em]{img/recalculation_color_red.png}
	\setlength{\abovecaptionskip}{15pt}
	\caption{Recalculation of only the first entry}
	\label{fig:coefRecalc_green}
	\end{subfigure}
	\begin{subfigure}[t]{0.30\textwidth}
	\includegraphics[width=1\linewidth,height=4em]{img/recalculation_color_green.png}
	\includegraphics[width=1\linewidth,height=4em]{img/recalculation_color_interpolated.png}
	\includegraphics[width=1\linewidth,height=4em]{img/recalculation_color_red.png}
	\setlength{\abovecaptionskip}{15pt}
	\caption{Recalculation of the interpolation of curves of both entries}
	\label{fig:coefRecalc_interpolated}
	\end{subfigure}
	\begin{subfigure}[t]{0.30\textwidth}
		\includegraphics[width=1\linewidth,height=4em]{img/recalculation_color_green.png}
		\includegraphics[width=1\linewidth,height=4em]{img/recalculation_color_fitRed.png}
		\includegraphics[width=1\linewidth,height=4em]{img/recalculation_color_red.png}
	\setlength{\abovecaptionskip}{15pt}
	\caption{Recalculation of only the second entry}
	\label{fig:coefRecalc_red}
	\end{subfigure}
	\caption{Comparison of coefficient recalculation techniques. Left: first fitted entry; right: second fitted entry; middle: result of recalculation}
	\label{fig:coefRecalculation}
\end{figure}


If a coefficient recalculation of an atlas lattice point with multiple fits is required, we interpolate the spectra of all of its fits and save the result with 3 coefficients. In terms of the smoothness of color gradients in the cube, this approach is more reliable and stable than if we were to pick only one of its fits and work with it. To support our claim and explain it more thoroughly, we run an experiment comparing these methods.

We seed a cube with two atlas entries with vastly distinct shapes of their reflectance spectra that, however, evaluate to neighboring voxels. This implies that there must exist an atlas lattice point that contains both coefficient representation of the seeded atlas entries. We 

 We pick one of the atlas lattice points that contains the coefficient representation of both of them, we pick a regular lattice points that is its neighbor, and we compare the spectra of its prior coefficients for all of these methods --- if recalculated from the first representation only (see~\cref{fig:coefRecalc_green}), from the second representation only (see~\cref{fig:coefRecalc_red}), and from their interpolation (see~\cref{fig:coefRecalc_interpolated}). For visualization purposes, we also include the colors these curves evaluate to under the FL11 illuminant. We present the results in~\cref{fig:coefRecalculation}.

If all the fits of an atlas lattice point contribute to the prior coefficients, the color difference between the result and the original fits is relatively similar for each fit. If, on the other hand, the point's prior coefficients come from one fit only, the reduced color difference between the point and the original fit comes at a cost of rather significant artifacts between the other entries. 

As we prefer to keep the cube's gradients as smooth as possible, we opt for the interpolation of fits.

Once the starting points are successfully fitted, we can use their coefficients as prior for other lattice points. We proceed similarly to the approach in~\cref{alg:upliftingAlgSigmoid}, where the lattice points are fitted in multiple \emph{fitting rounds}, each round attempting to fit the neighbors of the already fitted points. We provide a more detailed description of the principle behind our fitting algorithm in~\cref{alg:upliftingAlgMoments}.

\begin{algorithm}[t!]
	\caption{Fitting of the cube from starting points}
	\label{alg:upliftingAlgMoments}
	\begin{algorithmic}[1]
		\State $fittingRound \gets$ $0$
		\State $unfittedPoints \gets$ a list of all points in $RGBCube \setminus startingPoints$
		\ForAll{$point \in unfittedPoints$}
		\State $point.fittingDistance = MAX\_DOUBLE$
		\EndFor
		\While {$unfittedPoints$ is not empty}
		\State{$currRoundPts \gets$ points from $unfittedPoints$ that have at least one fitted neighbor}
		\ForAll{$point \in currRoundPts$}
		\ForAll{$fittedNeigbor \in $ fitted neighbors of $point$}
		\If{$fittedNeighbor \in atlasLatticePoint$}
		\State{$point.coefs \gets$ recalculateCoefs($fittedNeighbor.coefs$)} \label{algStep:coefficientRecomputation}
		\Else
		\State $point.coefs \gets fittedNeighbor.coefs$
		\EndIf 
		\State $[sDist,sCoefs]\gets$ CERES.Solve($point.coefs$, $costFunctions$)
		\If{$sDist \leq point.fittingDistance$}
		\State $point.fittingDistance \gets sDist$
		\State $point.coefs \gets sCoefs$
		\EndIf
		\If{$cDist \leq fittingThreshold$}
		\State $point.treated = true$
		\State break
		\EndIf
		\EndFor
		\If{$point.fittingDistance > fittingThreshold$ \textbf{or} $point$ has tried the coefficients of all of its neighbors}
		\State remove $point$ from $unfittedPoints$
		\EndIf
		\EndFor	
		\State $fittingRound \gets fittingRound+1$
		\EndWhile
	\end{algorithmic}
\end{algorithm}

(TOTO neviem ci tam pridat? ci to ukazat)The number of the fitting rounds depends both on the cube dimension and on the kind of atlas that has been used. If we choose not to input an atlas, the fitted cube ``grows'' from the middle, while seeding with an atlas makes the cube ``grow'' from many places, requiring a lot less round.

\subsection{Cube improvement} \label{ssec:cubeImprovement}

During our implementation of Borgtool, we use the ART 

Due to the shortcomings

To minimize the shortcomings of the optimizer mentioned in~\cref{ceresDeficiency}, we add a heuristic-based improvement of the coefficients, implemented in steps~\ref{algStep:heuristicsStart} through~\ref{algStep:heuristicsEnd}. Its implementation consists of solely slightly changing the first coefficient and running the optimizer again for a pre-defined number of times. We do not implement any other improvement, so as to not cause significant performance decrease. This is because we do not necessarily require the points to be fitted in the given round. Even if the fitting fails, it may still be successful in the following rounds, where the coefficients of newly fitted neighbors might be used as prior. Additionaly, after the cube fitting is done, we still run an ``improvement'' step for the unsuccessful points. We discuss the specifics of this step in~\cref{ssec:cubeImprovement}.

In extreme cases, such as when using a very low fitting threshold, the fitting of some points may be unsuccessful even after applying heuristic improvements. Usually, this happens for the outermost points of the cube, i.e. either (1,1,1), (0,0,0), or points with at least one of their coordinates set to either maximum or minimum. We therefore carry out the following improvements in the order as listed and terminate if any of them succeed: 
\begin{steps}
	\item utilize the same improvement method as mentioned in~\cref{par:improvementHeuristics}, but increase the number of iterations \label{step:improvement1}
	\item proceed in the same manner as in the previous step, but slightly alter all the coefficients instead of only the first \label{step:improvement2}
	\item pass the current best coefficients to the optimizer and keep improving until the optimizer is no longer able to
	\item if the point's distance to the (1,1,1) point in the cube is lower than it is to the (0,0,0) point, try setting the coefficients to $\{1, 0, 0, 0, \ldots\}$, as these are the coefficients for a constant curve obtaining the value 1 at every wavelength. Apply both~\ref{step:improvement1} and~\ref{step:improvement2} to these coefficients.\label{step:improvement4}
	\item if, on the other hand, the point is closer to the (0,0,0) point, use the $\{0, 0, 0 ,\ldots\}$ coefficients as prior and proceed as in~\ref{step:improvement4}
\end{steps}

We call this process \emph{cube improvement}. We once again emphasize that it is aimed only at the most extreme cases. As a matter of fact, there is a rather low chance of triggering it at all, and, even if it is triggered, the number of failed points to be improved is usually only one or two. Therefore, we do not concern ourselves with the performance of this process, as its runtime is negligible in comparison to the cube fitting.

We present the exact statistics on cube improvement in ref.

The issue that could arise with this process is that of the following interpolation. By using

The more important issue is the shape of the resulting curve. Using seemingly ``random'' coefficients as prior suggests the resulting curve's shape to be unlike those of its neighbors, which may result in metameric artifacts.

The reason behind using improvement heuristics both during and after fitting is both for improved time performance, i.e. a simple change of coefficients is less time-consuming that the process t

multithreaded 

We support from 2-8 moments then, however, we will talk about the number of coefficients (although user inserts moments). From now on we talk about coefficients. We do not support 1 or 2 coefs as they only create rovne ciary which does not reconstruct the color we want.

\subsection{Cube storage}
	
Once the cube is completely fitted, its contents are written to a binary file. As we want the resulting file to be as small as possible, we save only the information crucial for the purposes of rendering. Following, we provide a list of contents of a cube file:
\begin{itemize}
	\item \emph{version}
	\item \emph{features}, i.e. whether the cube contains debug information
	\item \emph{moment flag} --- a flag signifying that the cube is based on trigonometric moments. We extend the sigmoid cube structure with a sigmoid flag for an easier cube recognition in rendering software.
	\item \emph{dimension}
	\item \emph{illuminant} under which the cube has been fitted
	\item \emph{fitting threshold}
	\item for every point, we store:
	\begin{itemize}
		\item \emph{coefficient count}, which gives us information on whether the point is an atlas lattice point or a regular point
		\item \emph{coefficients}
		\item \emph{lattice RGB}
		\item \emph{fitting distance}, i.e. the distance between lattice and target RGB, should be below fitting threshold
		\item in case the debug information is included, the entry also contains:
		\begin{itemize}
			\item \emph{target RGB}
			\item \emph{treated variable}, i.e. in which round was the point fitted. If the variable is equal to $-1$, it means the fitting has failed for this point.
		\end{itemize}
	\end{itemize}
\end{itemize}

If we were to use such a cube for the purposes of rendering, the knowledge of both the entry's treated paramter and its target RGB is unnecessary. The only benefit the treated parameter provides is the capability to issue a warning in case the cube is incomplete. This can, however, also be achieved by spectral reconstruction from the coefficients and the comparison of its RGB to that of the entry's lattice point.

Storing the target RGB variable also provides no irreplaceable advantages. The target RGB of all the lattice points can simply be computed from the cube's dimension parameter, rendering such variables useless.

In addition to the storing of the cube, we also provide a function capable of loading such a cube and initializing its parameters. Borgtool utilizes this function in case the user wants to create a texture from an already existing cube or simply wants to check the correctness of the existing cube's parameters. The code of this function is also used during the integration in a rendering software.

\section{ART integration}

The user can choose the size of the cube etc. Borgtool has the following options, and we implement all of them for our method (except for something)
The borgtool already has the sigmoid method implemented as in algorithm daco
Uses three coefficients, fits from middle, and therefore results are metameric (show picture)
We implement the trigonometric moment method by changing up the code provided by peters
We aim for allowing the user to add a color atlas, however we also provide an option when the user fits from the middle
When fitted from middle, the optimizer works very similarly to the sigmoid method (show pictures)

Also explain the threshold value - it is really important to set it properly and that it affects performance.

This is because such an error evidently points out the insufficient cube size for the given atlas, either due to the atlas's large size or due to its concentration around certain colors (e.g. the Page 14 from the Munsell Book of Colors as in ukazka).

 as the interpolation of multiple spiky, non-similar spectra often results in a similarly uneven spectrum susceptible to metameric artifacts. Mention here that we NEED to have smooth spectra and that it is very important for interpolation. We therefore set parameters so that this is possible as this is the key. 
